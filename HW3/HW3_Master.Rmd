---
title: "Homework #3: Crime Prediction"
subtitle: "Data 621 Business Analytics and Data Mining"
author: "Aadi Kalloo, Nathan Lim, Asher Meyers, Daniel Smilowitz, Logan Thomson"
date: "Due July 3, 2016"
output: 
  pdf_document: 
    toc: true

geometry: margin=0.5in
---

```{r, echo = FALSE, message = FALSE, warning = FALSE}
library(knitr)
opts_chunk$set(warning=FALSE, message=FALSE, comment=NA, fig.align='center')

library(stringr)
library(pander)
library(ggplot2)
library(gridExtra)
library(reshape2)
library(MASS)
library(leaps)
library(pROC)
library(caret)

train_df <- read.csv(url("https://raw.githubusercontent.com/dsmilo/DATA621/master/HW3/Data/crime-training-data.csv"))
test_df <- read.csv(url("https://raw.githubusercontent.com/dsmilo/DATA621/master/HW3/Data/crime-evaluation-data.csv"))

train_df$target <- factor(train_df$target) # to factor

# Stuff for Part 1 table/figure preparation

```



#Data Exploration

```{r}
pairs(train_df, col= train_df$target)
```


#Data Preparation


#Model Creation

####Model 1
```{r, echo = FALSE, fig.height=2}
model1 <- glm(target ~ nox + age + rad + medv, family=binomial, data = train_df)
summary(model1)

train_df$predicted_model1 <- predict(model1, train_df, type='response')
train_df$target_model1 <- ifelse(train_df$predicted_model1>0.5, 1, 0)
roc_model1 <- pROC::roc(target ~ predicted_model1, data = train_df)

plot(roc_model1, col="red")

confusionMatrix(train_df$target, train_df$target_model1, positive = "1")
# AUC is  0.957 Accuracy : 0.8691 

```


####Model 2
```{r, echo = FALSE, fig.height=2}
model2 <- glm(target ~ nox + age + rad + ptratio + medv, family=binomial, data = train_df)
summary(model2)

train_df$predicted_model2 <- predict(model2, train_df, type='response')
train_df$target_model2 <- ifelse(train_df$predicted_model2>0.5, 1, 0)

roc_model2 <- pROC::roc(factor(target) ~ predicted_model2, data=train_df)

plot(roc_model2, col="red")

confusionMatrix(train_df$target, train_df$target_model2, positive = "1")
#AUC is 0.9605 Accuracy : 0.8691
```

####Model 3
```{r, echo = FALSE, fig.height=2}
model3 <- glm(target ~ log(nox) + age + log(rad) + medv, family=binomial, data = train_df)
summary(model3)

train_df$predicted_model3 <- predict(model3, train_df, type='response')
train_df$target_model3 <- ifelse(train_df$predicted_model3>0.5, 1, 0)
roc_model3 <- pROC::roc(factor(target) ~ predicted_model3, data=train_df)

plot(roc_model3, col="red")
confusionMatrix(train_df$target, train_df$target_model3, positive = "1")

#AUC is 0.9584, Accuracy : 0.8691 
```

#Model4
```{r, echo = FALSE, fig.height=2}
model4 <- glm(target ~ log(nox) +  log(rad) + tax, family=binomial, data = train_df)
summary(model4)
train_df$predicted_model4 <- predict(model4, train_df, type='response')
train_df$target_model4 <- ifelse(train_df$predicted_model4>0.5, 1, 0)
roc_model4 <- pROC::roc(factor(target) ~ predicted_model4, data=train_df)

plot(roc_model4, col="red")
confusionMatrix(train_df$target, train_df$target_model4, positive = "1")

#AUC is 0.961,  Accuracy : 0.8648
```


#Model Selection and Prediction

The model selected used only the significant predictors (Model 3) was selected as the best model for prediction of `TARGET` in the crime data set.  While the AUC value of this model the second-highest of the four models tested, its mean cross-validation error indicates that it is has the best predictive value for unseen data.  Additionally, it is a parsimonious model, and the simplicity lends itself to easier understanding of the model by other users.


##10-fold Cross Validation
```{r, echo = FALSE}
k = 10
set.seed(1306)
folds = sample(1:k, nrow(train_df), replace = TRUE)
cv.errors1 = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))
cv.errors2 = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))
cv.errors3 = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))
cv.errors4 = matrix(NA, k, 10, dimnames = list(NULL, paste(1:10)))

train_df$target = as.numeric(as.character(train_df$target))

for (j in 1:k) {
  
  model1 <- glm(target ~ nox + age + rad + medv, family = binomial, data = train_df[folds != j, ])
  model2 <- glm(target ~ nox + age + rad + ptratio + medv, family = binomial, data = train_df[folds != j, ])
  model3 <- glm(target ~ log(nox) + age + log(rad) + medv, family = binomial, data = train_df[folds != j, ])
  model4 <- glm(target ~ log(nox) +  log(rad) + tax, family = binomial, data = train_df[folds != j, ])
  
    #best.fit = regsubsets(y ~ ., data = train_df[folds != j, ], nvmax = 10)
    for (i in 1:10) {
      pred1 = predict(model1, train_df[folds == j, ], id = i)
      cv.errors1[j, i] = mean((train_df$target[folds == j] - pred1) ^ 2)
    
      pred2 = predict(model2, train_df[folds == j, ], id = i)
      cv.errors2[j, i] = mean((train_df$target[folds == j] - pred2) ^ 2)
  
      pred3 = predict(model3, train_df[folds == j, ], id = i)
      cv.errors3[j, i] = mean((train_df$target[folds == j] - pred3) ^ 2)
      
      pred4 = predict(model4, train_df[folds == j, ], id = i)
      cv.errors4[j, i] = mean((train_df$target[folds == j] - pred4) ^ 2)
  }
  
}


#cv.errors1
mean.cv.errors1 <- apply(cv.errors1, 2, mean)
#mean.cv.errors1 = apply(cv.errors1, 2, mean)

# which.min(mean.cv.errors1)
# mean.cv.errors1[6]
#cv.errors2
mean.cv.errors2 <- apply(cv.errors2, 2, mean)
#mean.cv.errors2
# which.min(mean.cv.errors2)
# mean.cv.errors2[6]


#cv.errors3
mean.cv.errors3 <- apply(cv.errors3, 2, mean)
#mean.cv.errors3
# which.min(mean.cv.errors3)
# mean.cv.errors3[6]


#cv.errors4
mean.cv.errors4 <- apply(cv.errors4, 2, mean)
#mean.cv.errors4
# which.min(mean.cv.errors4)
# mean.cv.errors4[6]

all.cv.error = data.frame(
mean(mean.cv.errors1),
mean(mean.cv.errors2),
mean(mean.cv.errors3),
mean(mean.cv.errors4) )
names(all.cv.error) = c("Model1", "Model2", "Model3", "Model4")
#all.cv.error
all.cv.error = t(all.cv.error)
names(all.cv.error) = c("Model", "Mean CV Error")


```
#### Model 5

Like Model 1, this is formed using best subsets regression with BIC as a criterion for choosing the number of predictors; however, with this model, predictors are removed sequentially until all predictors are statistically significant (p < 0.05). This leads to the removal of age and median home values as predictors.
```{r, echo = FALSE, fig.height=2}
model5 <- glm(target ~ nox+rad, data = train, family = binomial(link='logit'))
summary(lmminbic)                           # Summary of the linear regression model

train_df$predicted_model5 <- predict(model5, train_df, type = 'response')
train_df$target_model5 <- ifelse(train_df$predicted_model5>0.5, 1, 0)
roc_model5 <- pROC::roc(factor(target) ~ predicted_model5, data=train_df)

plot(roc_model5, col="red")
confusionMatrix(train_df$target, train_df$target_model5, positive = "1")
```

#####Mean CV Error
```{r, echo = FALSE}
pander(all.cv.error)
```


The linear model is applied to an evaluation dataset containing response variables for 259 cases.  A table of the predicted team wins is presented below.

```{r evaluation, echo=FALSE}
#evaluation_data <- read.csv('https://raw.githubusercontent.com/dsmilo/DATA621/master/HW1/data/moneyball-evaluation-data.csv')
par(mfrow = c(1, 2), pin = c(2, 2))

predicted_wins <- predict(model3, train_df, type='response')
predicted_wins_bin = ifelse(predicted_wins > 0.5, 1, 0)
pander(table(predicted_wins_bin))
training_wins = train_df$target
pander(table(training_wins))
```

Similar to the training dataset, the predictions for the test data set predictions are weighted more toward crime being below the median

A comparison of the full sets of predictions for the evaluation dataset is available in Appendix B.

\newpage 

#Appendix A 
```{r, echo = FALSE}

```

\newpage

#Appendix B -- Index-wise Results from Predictive Model
```{r, echo = FALSE}
# appendixB = data.frame(matrix(NA, nrow = 130, ncol = 4))
# appendixB[, 1] = evaluation_data$INDEX[1:130]
# appendixB[, 2] = predicted_wins[1:130] 
# appendixB[, 3] = c(evaluation_data$INDEX[131:259], NA)
# appendixB[, 4] = c(predicted_wins[131:259], NA)
# #appendixB = appendixB[-130:259,]
# names(appendixB) = c("Index", "Predicted Value", "Index", "Predicted Value")
# 
# pander(appendixB)
```

\newpage

#Appendix C -- R Code

```{r, echo = TRUE, eval = FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}

```
