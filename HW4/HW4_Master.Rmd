---
title: "Homework #4: Insurance Claim Prediction"
subtitle: "Data 621 Business Analytics and Data Mining"
author: "Aadi Kalloo, Nathan Lim, Asher Meyers, Daniel Smilowitz, Logan Thomson"
date: "Due July 10, 2016"
output: 
  pdf_document: 
    toc: yes
    number_sections: yes
geometry: margin=0.5in
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE, comment=NA, fig.align='center')

library(stringr) #For string functions
library(glmnet) #For binary logistic regression
library(leaps) #For best subsets
library(pROC) #For ROC curve
library(car)
library(MASS)
library(ROCR)
library(ggplot2)
library(stringr)
library(dplyr)
library(reshape)
library(glmnet)
library(leaps)
library(pROC)
library(MASS)
library(ggplot2)
library(vcd)
library(pander)
library(tidyr)
library(e1071)

train <- read.csv("https://raw.githubusercontent.com/dsmilo/DATA621/master/HW4/Data/insurance_training_data.csv")
```

\newpage

# Data Exploration
The dataset of interest contains information about customers of an auto insurance company.  The dataset has 8161 rows (each representing a customer) and 25 variables.  There are 23 predictor variables and 2 response variables: `TARGET_FLAG`, a binary categorical variable representing whether each customer has been in an accident; and `TARGET_AMT`, a numerical variable indicating the cost of a crash that a customer was in.  The class of variables read in from the dataset is presented below:

```{r data-class}
var_class <- data.frame(Class = rep(NA, ncol(train) - 1), Levels = rep(NA, ncol(train) - 1), stringsAsFactors = FALSE, check.names = FALSE, row.names = names(train)[-1])
for(i in 2:ncol(train)) {
  var_class[i - 1, 1] <- class(train[, i])
  var_class[i - 1, 2] <- ifelse(length(levels(train[, i])) == 0, '-', length(levels(train[, i])))
}
pander(var_class)
```

The very high number of levels for four of the variables (`INCOME`, `HOME_VAL`, `BLUEBOOK`, and `OLDCLAIM`) indicates that these variables are not in fact factors; investigation of the dataset indicates that these are dollar values interpreted as strings due to the presence of dollar signs and commas.  The numerical values are extracted for these variables.

```{r numeric}
#Remove all $ signs and commas, i.e. from INCOME, HOME_VAL, BLUEBOOK, OLDCLAIM
train$INCOME <- extract_numeric(train$INCOME)
train$HOME_VAL <- extract_numeric(train$HOME_VAL)
train$BLUEBOOK <- extract_numeric(train$BLUEBOOK)
train$OLDCLAIM <- extract_numeric(train$OLDCLAIM)
```

Additionally, there are 7 variables with only two levels.  These are recast as binary variables as follows:

  * `PARENT1`, `MSTATUS`, `RED_CAR`, and `REVOKED`: using 1 to indicate Yes
  * `SEX`: using 1 to indicate Male
  * `CAR_USE`: using 1 to indicate Commercial
  * `URBANICITY`: using 1 to indicate Highly Urban/ Urban

Finally, there are three categorical variables -- factors with more than two levels.  Dummy variables are created for each of these, as follows:

  * `EDUCATION`: using High School as the base case
  * `CAR_TYPE`: using Minivan as the base case
  * `JOB`: using the blank value as the base case
  
```{r binary-dummy}
#Convert indicator variables to 0s and 1s; 1 = Yes, Male for Sex, Commercial for Car Use, Red for RED_CAR, and Highly Urban for URBANICITY
train$PARENT1 <- ifelse(train$PARENT1=="Yes", 1, 0)
train$MSTATUS <- ifelse(train$MSTATUS=="Yes", 1, 0)
train$SEX <- ifelse(train$SEX=="M", 1, 0)
train$CAR_USE <- ifelse(train$CAR_USE=="Commercial", 1, 0)
train$RED_CAR <- ifelse(train$RED_CAR=="yes", 1, 0)
train$REVOKED <- ifelse(train$REVOKED=="Yes", 1, 0)
train$URBANICITY <- ifelse(train$URBANICITY == "Highly Urban/ Urban", 1, 0)

#Convert categorical predictor values to indicator variables - EDUCATION, CAR_TYPE, JOB

#EDUCATION, High school graduate is base case
train$HSDropout <- ifelse(train$EDUCATION=="<High School", 1, 0)
train$Bachelors <- ifelse(train$EDUCATION=="Bachelors", 1, 0)
train$Masters <- ifelse(train$EDUCATION=="Masters", 1, 0)
train$PhD <- ifelse(train$EDUCATION=="PhD", 1, 0)

#CAR_TYPE, base case is minivan
train$Panel_Truck <- ifelse(train$CAR_TYPE=="Panel Truck", 1, 0)
train$Pickup <- ifelse(train$CAR_TYPE=="Pickup", 1, 0)
train$Sports_Car <- ifelse(train$CAR_TYPE=="Sports Car", 1, 0)
train$Van <- ifelse(train$CAR_TYPE=="Van", 1, 0)
train$SUV <- ifelse(train$CAR_TYPE=="z_SUV", 1, 0)

#JOB, base case is ""
train$Professional <- ifelse(train$JOB == "Professional", 1, 0)
train$Blue_Collar <- ifelse(train$JOB == "Professional", 1, 0)
train$Clerical <- ifelse(train$JOB == "Clerical", 1, 0)
train$Doctor <- ifelse(train$JOB == "Doctor", 1, 0)
train$Lawyer <- ifelse(train$JOB == "Lawyer", 1, 0)
train$Manager <- ifelse(train$JOB == "Manager", 1, 0)
train$Home_Maker <- ifelse(train$JOB == "Home Maker", 1, 0)
train$Student <- ifelse(train$JOB == "Student", 1, 0)

train <- train %>% dplyr::select(-c(INDEX,EDUCATION,CAR_TYPE,JOB))
```

\pagebreak

A summary of each variable is presented below:

```{r summary}
means <- sapply(train, function(y) mean(y, na.rm = TRUE))
medians <- sapply(train, function(y) median(y, na.rm = TRUE))
IQRs <- sapply(train, function(y) IQR(y, na.rm = TRUE))
skews <- sapply(train, function(y) skewness(y, na.rm = TRUE))
cors_flag <- as.vector(cor(train$TARGET_FLAG, train[,1:ncol(train)], use = "complete.obs"))
cors_amt <- as.vector(cor(train$TARGET_AMT, train[,1:ncol(train)], use = "complete.obs"))
NAs <- sapply(train, function(y) sum(length(which(is.na(y)))))

datasummary <- data.frame(means, medians, IQRs, skews, cors_flag, cors_amt, NAs)
colnames(datasummary) <- c("MEAN", "MEDIAN", "IQR", "SKEW", "$r_{FLAG}$", "$r_{AMT}$", "NAs")
datasummary <- round(datasummary, 2)
pander(datasummary)
```

From the table above, it is clear that there are four variables with missing values, with the proportion of values missing ranging from less than < 0.1% to roughly 6.2%; these missing values will need to either be imputed or excluded from the dataset before modeling.  The variables exhibit varying levels of skewness, with a few extreme values.

The large number of binary variables in the dataset makes graphical visualization of the distribution of all variables not particularly useful.  The proportion of binary variables having a value of 0 or 1 is presented in the table below:

\pagebreak

```{r binary-tables}
binaries <- train %>% dplyr::select(-c(TARGET_AMT, KIDSDRIV, AGE, HOMEKIDS, YOJ, INCOME, HOME_VAL, TRAVTIME, BLUEBOOK, TIF, OLDCLAIM, CLM_FREQ, MVR_PTS, CAR_AGE))

binarytable <- t(sapply(binaries, table)/nrow(binaries))
binarytable <- round(binarytable, 2)
pander(binarytable)
```

The remaining variables are vizualized below in boxplots:

```{r box_plots, fig.height=6}
non_binary <- train %>% dplyr::select(c(TARGET_AMT, KIDSDRIV, AGE, HOMEKIDS, YOJ, INCOME, HOME_VAL, TRAVTIME, BLUEBOOK, TIF, OLDCLAIM, CLM_FREQ, MVR_PTS, CAR_AGE))
non_binary <- melt(non_binary)
ggplot(non_binary, aes(variable, value)) + geom_boxplot(aes(fill = variable), alpha = 0.75, show.legend = FALSE) + facet_wrap(~variable, scale="free") + scale_y_continuous('') + scale_x_discrete('', breaks = NULL) + ggtitle("Distribution of Predictor and Target Variables\n")
```

The boxplots illustrate the high skewness of the distributions of the predictors `KIDSDRIV`, `INCOME`, `HOME_VAL`, `TRAVTIME`, `BLUEBOOK`, `TIF`, `OLDCLAIM` and `MVR_PTS`.  The target `TARGET_AMT` is also highly skewed -- this makes sense, as this value is 0 for any customers without claims.  Density plots of these variables are presented below:

```{r density-plots}
skewed <- train %>% dplyr::select(c(TARGET_AMT, KIDSDRIV, INCOME, HOME_VAL, TRAVTIME, BLUEBOOK, TIF, OLDCLAIM, MVR_PTS))
skewed <- melt(skewed)
ggplot(skewed, aes(value)) + geom_density(aes(fill = variable, col = variable), alpha = 0.5, show.legend = FALSE) + facet_wrap(~variable, scale="free") + scale_y_continuous('', breaks = NULL) + scale_x_continuous('', breaks = NULL) + ggtitle("Density of Skewed Variables\n")
```

Since `TARGET_AMT` and `OLDCLAIM` have such high concentrations at values of zero, separate plots for these two variables are created with values of zero removed:

```{r claim-density, fig.height=3}
ggplot(subset(skewed, (variable == "TARGET_AMT" & value != 0) | (variable == "OLDCLAIM" & value !=0)), aes(value)) + geom_density(aes(fill = variable, col = variable), alpha = 0.5, show.legend = FALSE) + facet_wrap(~variable, scale = "free") + scale_y_continuous('', breaks = NULL) + scale_x_continuous('') + ggtitle("Density of Non-Zero Claim Amounts\n")
```

The 8 predictors with the highest correlation to `TARGET_FLAG` and the 8 predictors with the highest correlation to `TARGET_AMT` share 7 predictors.  The correlation between these variables is investigated and plotted below:

```{r predictor-pairs, fig.height=7, fig.width=7}
pairs(~MVR_PTS+CLM_FREQ+URBANICITY+HOME_VAL+PARENT1+CAR_USE+OLDCLAIM, data=train, main="Predictors with High Correlattions to Targets", col="slategrey")
```

There appears to be evidence of possible multicollinearity between `HOME_VAL` and `OLDCLAIM`.

Finally, boxplots are also prepared for non-binary variables split by `TARGET_FLAG`:

```{r boxplot_by_target, fig.height=6}
numeric <- train %>% dplyr::select(c(TARGET_FLAG, TARGET_AMT, KIDSDRIV, AGE, HOMEKIDS, YOJ, INCOME, HOME_VAL, TRAVTIME, BLUEBOOK, TIF, OLDCLAIM, CLM_FREQ, MVR_PTS, CAR_AGE))

numeric <- melt(numeric, id.vars="TARGET_FLAG")
numeric$TARGET_FLAG <- factor(numeric$TARGET_FLAG)
ggplot(numeric, aes(TARGET_FLAG, value)) + geom_boxplot(aes(fill = TARGET_FLAG), alpha = 0.5) + facet_wrap(~variable, scale="free") + scale_fill_discrete(guide = FALSE) + scale_y_continuous('', labels = NULL, breaks = NULL) + scale_x_discrete('') + ggtitle("Distribution of Predictors by TARGET_FLAG\n")
```

Interestingly, there is not an immediately visible difference in median value based on `TARGET_FLAG` values, while the range of predictor values for each flag value differs noticeably.

# Data Preparation



# Model Creation

## Multiple Linear Regression


## Binary Logistic Regression



# Model Selection & Prediction





\newpage 

# Appendix A: Index-wise Results from Predictive Model {-}
```{r appendix-a, eval=FALSE}
appendixA = data.frame(matrix(NA, nrow = 2141, ncol = 4))


names(appendixA) = c("Index", "Predicted Probability", "Predicted Classifcation", "Predicted Cost")

pander(appendixA)
```

\newpage

# Appendix B: R Code {-}

```{r appendix-b, eval=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=60)}

```
