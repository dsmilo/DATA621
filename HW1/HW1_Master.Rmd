---
title: "Homework #1: Baseball Analysis"
subtitle: "Data 621 Business Analytics and Data Mining"
author: "Aadi Kalloo, Nathan Lim, Asher Meyers, Daniel Smilowitz, Logan Thomson"
date: "Due June 19, 2016"
output: pdf_document
geometry: margin=0.5in
---

```{r, echo = FALSE}
library(stringr)
library(pander)
library(knitr)
library(ggplot2)
library(gridExtra)

trainingdata = read.csv("https://raw.githubusercontent.com/aadikalloo/AadiMSDA/master/IS621-Data-Mining/moneyball-training-data.csv")

createSummaryTable <- function(trainingdata1) {
  ####### Mean and Medians Table
  mean_median_df = data.frame(matrix(0, nrow = ncol(trainingdata1), ncol = 2))
  
  mean2 <- function(x)   {mean(x, na.rm = TRUE)}
  median2 <- function(x) {median(x, na.rm = TRUE)}
  
  means = as.data.frame(lapply(trainingdata1, mean2))
  medians = as.data.frame(lapply(trainingdata1, median2))
  lengths = as.data.frame(lapply(trainingdata1, length))
  
  mean_median_df[, 1] = names(means)
  mean_median_df[, 2] = t(means[1, ])
  mean_median_df[, 3] = t(medians[1, ])
  #mean_median_df[, 4] = t(lengths[1, ])
  
  names(mean_median_df) = c("VAR_NAME", "MEAN", "MEDIAN")
  #kable(mean_median_df, digits = 2)
  ####################################################
  
  ########Correlations to Wins
  
  cor_df = data.frame(matrix(0, nrow = ncol(trainingdata1) - 2, ncol = 2))
  
  cors = as.data.frame(cor(trainingdata1$TARGET_WINS, trainingdata1[, 3:ncol(trainingdata1)], use = "pairwise.complete.obs"))
  cor_df[, 1] = names(cors)
  cor_df[, 2] = t(cors[1, ])
  
  names(cor_df) = c("VAR_NAME", "CORRELATION TO WINS (r)")
  
  #kable(cor_df, digits = 2)
  ####################################################
  
  ########Missing Values per variable
  mv_df = data.frame(matrix(0, nrow = ncol(trainingdata1), ncol = 2))
  
  num_missing <- function(x)   {sum(is.na(x))}
  
  
  missingvalues = as.data.frame(lapply(trainingdata1, num_missing))
  mv_df[, 1] = names(missingvalues)
  mv_df[, 2] = t(missingvalues[1, ])
  
  names(mv_df) = c("VAR_NAME", "NUM_MISSING")
  
  #kable(mv_df, digits = 2)
  ####################################################
  
  
  data_exp = merge(mean_median_df, cor_df, by.x = "VAR_NAME", by.y = "VAR_NAME")
  data_exp = merge(data_exp, mv_df, by.x = "VAR_NAME", by.y = "VAR_NAME")
  temp = as.data.frame(cbind(mean_median_df[2,], NA, NA))
  names(temp) = names(data_exp)
  data_exp = rbind(temp, data_exp)
}


trainingdata_bk = trainingdata
```



#Data Exploration

The data analyzed in this report includes 2276 professional baseball teams for the years 1871-2006. In total, 16 variables were present in the data provided. Included below is a summary of descriptive statistics, correlations to wins, and the number of missing values for each variable in the provided data set:

```{r, echo = FALSE}
data_exp = createSummaryTable(trainingdata)
kable(data_exp)
```

Below are graphs that show the relationship to _Target Wins_ for the three variables with the highest correlation coefficient: 

```{r, echo = FALSE}
par(mfrow = c(1, 3))
top3correlations = c(0,0,7,4,0,0,6)
for (plot_count in c(3, 4, 7)) {
  plot(x = trainingdata[, plot_count], y = trainingdata$TARGET_WINS, xlab = names(trainingdata)[plot_count], ylab = "Target Wins", main = paste0(names(trainingdata)[plot_count], " r = ", round(data_exp[top3correlations[plot_count], 4], 3) ))
}
```

The full array of correlations graphs may be found in Appendix A. 
<!-- -->

#Data Preparation

It was determined that the _Hits By Pitch_ variable had too many missing values to be useful for regression, and thus this variable was excluded from the model building process. As shown in Table 1 above, there are several variables that have missing values. The attempted solution to this problem involved imputation using the median for each variable in the data set. A summary of the data is shown here again for inspection and confirmation of similarity between the old and new data sets:


#####Missing Values Imputed With Median
```{r, echo = FALSE}
trainingDataRaw = trainingdata
data_no_index <- trainingDataRaw[,c(2:17)] # delete index

for (i in 1:16) {
  data_no_index[,i][is.na(data_no_index[,i])] = median(data_no_index[,i], na.rm = TRUE)
}

df_new=data_no_index
#summary(df_new)
imp_data = createSummaryTable(df_new)
kable(imp_data)
trainingdata = df_new
```

The dataset contains 17 columns - an index column (INDEX), a response column (TARGET_WINS) and 15 predictor columns. There are 2,276 observations - but there are many missing values for many of the predictors. 

Two predictors in particular stand out:
```{r, echo = FALSE}
a = c("TEAM_BATTING_HBP", "Batters hit by pitch (free base)", "Positive", "91.6%", "0.07", "0.31")
b = c("TEAM_BASERUN_CS", "Strikeouts by batters", "Negative", "33.9%", "0.02", "0.39")

names(a) = c("Predictor Name", "Description", "Impact", "% Missing", "r with Response", "p-Value")
names(b) = names(a)
c = as.data.frame(rbind(a, b))
kable(c)
```

Including these predictors in our dataset would mean that we would either have to a) forgo a significant chunk of our data (34% or 92%) or b) impute a large number of data points.  Their correlation coefficients with the response are less than an absolute value of 7%; the p values of a simple one variable linear regression using them and the response yields models of no statistical significance (i.e. p > 0.05). Thus, it seems safe to exclude these predictors from our models. This way, we avoid the twin pitfalls of mass exclusion and imputation.

Further exclusions to the data were made:

Exclusion | Explanation
---------------------------|---------------------------------------------
INDEX == 1347 | This row had a suspicious set of zero entries
TEAM_BATTING_BB == 0 | Anomalously low walk count (expected occurences of a zero value for this predictor are zero)
TEAM_BATTING_SO | Outside of recognized records [link](http://www.baseball-almanac.com/recbooks/rb_strike2.shtml)
TEAM_BATTING_HR | Outside of recognized records [link](http://www.baseball-almanac.com/recbooks/rb_hr7.shtml)

It should be noted that the records excluded from the first two rows of the table above are the same exact points (which would technically make the second exclusion redundant...). That suggests that for whatever reason, strikeouts were not recorded for those rows, but were marked as zero. Those two predictors have the same number of NA values, 102, suggesting their recording method was linked somehow.

#Model Creation

```{r, echo = FALSE}
##### LOAD DATA

trainingDataRaw = trainingdata_bk
data_no_index = trainingDataRaw[,2:17] # delete index
```

####Model --(Nathan)

```{r}

g1 <- ggplot(df_new, aes(x=TEAM_FIELDING_E)) + geom_histogram(binwidth = 10)
g2 <- ggplot(df_new, aes(x=log(TEAM_FIELDING_E))) + geom_histogram(binwidth = 0.05)
grid.arrange(g1, g2, ncol=2)

model2<- lm(TARGET_WINS ~  TEAM_BATTING_H + TEAM_BASERUN_SB + TEAM_FIELDING_DP + 
              log(TEAM_FIELDING_E), df_new)

par(mfrow=c(2,2)); plot(model2)
par(mfrow=c(1,1)); plot(model2$residuals)
summary(model2)


df_new$residuals <- model2$residuals
df_new$predicted <- model2$fitted.values

ggplot(df_new, aes(x=TARGET_WINS, y=predicted)) +
  geom_point() + stat_smooth(method="lm")

```





#####Model 1

Description:



Relevant code for checking correlation coefficients and p values:

```{r}
#dfraw <- read.csv(url("https://raw.githubusercontent.com/dsmilo/DATA621/master/HW1/data/moneyball-training-data.csv"))
dfraw = trainingdata
dfHBP <- dfraw[!is.na(dfraw$TEAM_BATTING_HBP),] #Create df without null values for TEAM_BATTING_HBP
paste0("correlation coefficient between response and TEAM_BATTING_HBP: ", cor(dfHBP$TARGET_WINS,dfHBP$TEAM_BATTING_HBP)) #Calculate correlation coefficient between response and TEAM_BATTING_HBP
summary(lm(TARGET_WINS~TEAM_BATTING_HBP, dfHBP))#See summary of linear regression model using TEAM_BATTING_HBP

dfCS <- dfraw[!is.na(dfraw$TEAM_BASERUN_CS),]#Create df without null values for TEAM_BASERUN_CS
paste0("correlation coefficient between response and TEAM_BASERUN_CS: ", cor(dfCS$TARGET_WINS,dfCS$TEAM_BASERUN_CS)) #Calculate correlation coefficient between response and TEAM_BASERUN_CS
summary(lm(TARGET_WINS~TEAM_BASERUN_CS, dfCS))#See summary of linear regression model using TEAM_BASERUN_CS
```

I then created a linear regression, and created additional, improved regression models by removing predictors with low significance, until the f-statistic of the regression stopped increasing with the removal of predictors. [The f-stats mentioned in the comments may have changed]

```{r}
#dfraw <- read.csv(url("https://raw.githubusercontent.com/dsmilo/DATA621/master/HW1/data/moneyball-training-data.csv"))
dfraw = trainingdata_bk
dfremove <- subset(dfraw, INDEX == 1347 | TEAM_BATTING_BB == 0 | 
                     TEAM_BATTING_3B < 11 | TEAM_BATTING_3B > 153 | # http://www.baseball-almanac.com/rb_trip2.shtml
                     TEAM_BATTING_HR < 3 | TEAM_BATTING_HR > 264 |#http://www.baseball-almanac.com/recbooks/rb_hr7.shtml
                     TEAM_PITCHING_SO > 1781 | #http://www.baseball-almanac.com/recbooks/rb_strik.shtml
                     TEAM_BATTING_SO < 308 | TEAM_BATTING_SO > 1535 #http://www.baseball-almanac.com/recbooks/rb_strike2.shtml
                   )$INDEX 
#length(dfremove)
df <- subset(dfraw, !(INDEX %in% dfremove))
#str(df)
df <- df[, -c(1,10,11,15)] #Remove caught stealing and hit by pitcher variables, and pitching strikeouts.
#View(df)
#View(df1)
#summary(df)
#str(df)

fit <- lm(TARGET_WINS~.,df)
summary(fit)
fit1 <- update(fit, .~.-TEAM_BATTING_H)
summary(fit1)
fit2 <- update(fit1, .~.-TEAM_PITCHING_HR)
summary(fit2)
fit3 <- update(fit2, .~.-TEAM_BATTING_2B)
summary(fit3) #F stat of 130
fit4 <- update(fit3, .~.-TEAM_PITCHING_BB) 
summary(fit4)
fit5 <- update(fit4, .~.-TEAM_PITCHING_H)
summary(fit5)
fit6 <- update(fit5, .~.-TEAM_FIELDING_DP) #Wrong sign on predictor Fielding
summary(fit6)

#Correlation Matrix
#View(round(cor(df), 2))

#These are variables that I tried but didn't turn out to be valuable
df$TEAM_BATTING_1B <- df$TEAM_BATTING_H - df$TEAM_BATTING_2B - df$TEAM_BATTING_3B - df$TEAM_BATTING_HR #Singles - 1st Base Hits
df$TEAM_BATTING_HRP <- df$TEAM_BATTING_HR/df$TEAM_BATTING_H #Home runs as a percentage of base hits
df$TEAM_BATTING_HSO <- df$TEAM_BATTING_H/df$TEAM_BATTING_SO #Ratio of hits to strikeouts
```


#####Create a linear model using all predictors. The INDEX column is excluded.

```{r}
FullModel <- lm(TARGET_WINS ~.-INDEX, trainingDataRaw)
summary(FullModel) #Summary of full model
```

#####Put full model through stepwise regression, where predictors with less significance are sequentially removed.

```{r}
stepFull <- step(FullModel) 
summary(stepFull)
```


```{r}
#####Generate predictions using the stepFull model
predictionsStepFull <- predict(stepFull, trainingDataRaw) 
#View(predictionsStepFull)
```

#####Generate the RMSE of the stepFull model
```{r}
rmseStep <- sqrt(mean((trainingDataRaw$TARGET_WINS[!is.na(predictionsStepFull)] - predictionsStepFull[!is.na(predictionsStepFull)])^2)) 
rmseStep
```

```{r}
par(mfrow=c(2,2)) #Set up a four panel plot for evaluating regression
plot(stepFull) #Displays Residuals vs Fitted, Scale-Location,  and Normal Q-Q.
```

#####Evaluation of Stepwise model without TEAM_BATTING_HBP
```{r}
ReducedModel <- lm(TARGET_WINS ~., trainingDataRaw[,c(2:10, 12:17)])
summary(ReducedModel)
stepReduced <- step(ReducedModel)
predictionsStepReduced <- predict(stepReduced, trainingDataRaw[,c(2:10, 12:17)])
rmseStepR <- sqrt(mean((trainingDataRaw$TARGET_WINS[!is.na(predictionsStepReduced)] - predictionsStepReduced[!is.na(predictionsStepReduced)])^2))
rmseStepR
```


## Aadi Models

```{r}
#all variables that have positive impact
modeldata_positive = trainingdata_bk[, c(2, 3:7, 9, 15, 17)]  #HBP not included
model3 = lm(TARGET_WINS ~., data = modeldata_positive)
summary(model3)

#all variables that have negative impact
modeldata_negative = trainingdata_bk[, c(2, 8, 10, 12:14, 16)]
model4 = lm(TARGET_WINS ~., data = modeldata_negative)
summary(model4)

#all batting variables
modeldata_batting = trainingdata_bk[, c(2, 3:7, 8)] #HBP not included
model5 = lm(TARGET_WINS ~., data = modeldata_batting)
summary(model5)

#all non batting variables
modeldata_notbatting = trainingdata_bk[, c(2, 9:10, 12:17)]
model6 = lm(TARGET_WINS ~., data = modeldata_notbatting)
summary(model6)

#most highly correlated with target wins
modeldata_hc = trainingdata_bk[,subset(data_exp, data_exp[, 4] > 0.15)$VAR_NAME]
modeldata_hc = cbind(trainingdata_bk$TARGET_WINS, modeldata_hc)
names(modeldata_hc)[1] = "TARGET_WINS"
model7 = lm(TARGET_WINS ~., data = modeldata_hc)
summary(model7)

modeldata_hc = trainingdata_bk[,subset(data_exp, data_exp[, 4] < 0)$VAR_NAME]
modeldata_hc = cbind(trainingdata_bk$TARGET_WINS, modeldata_hc)
names(modeldata_hc)[1] = "TARGET_WINS"
model8 = lm(TARGET_WINS ~., data = modeldata_hc)
summary(model8)

completecases = trainingdata_bk[complete.cases(trainingdata_bk),]
model9 = lm(TARGET_WINS ~., data = completecases)
summary(model9)
```


#Model Selection and Prediction