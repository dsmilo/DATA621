---
title: "DATA621_HW1_LoganT"
author: "Logan Thomson"
date: "June 18, 2016"
output: html_document
---

```{r}
# Load packages
library("ggplot2")
library("reshape2")
library("MASS")
```

```{r}
# Load data
dfraw <- read.csv(url("https://raw.githubusercontent.com/dsmilo/DATA621/master/HW1/data/moneyball-training-data.csv"))

data_no_index <- dfraw[ ,c(2:17)] #Remove INDEX column

for (i in 1:16) { # Impute median for missing values
  data_no_index[,i][is.na(data_no_index[,i])] <-  median(data_no_index[,i], na.rm = TRUE)
}
df_new <- data_no_index
summary(df_new)
```

```{r}
# calculated columns for new predictors

# Get Singles from Base Hits
df_new$TEAM_BATTING_1B <- with(df_new, TEAM_BATTING_H - TEAM_BATTING_2B - TEAM_BATTING_3B - TEAM_BATTING_HR)

# Create Extra Bases and Total Bases predictors
df_new$TEAM_BATTING_XTRA_BASE <- with(df_new, TEAM_BATTING_2B + TEAM_BATTING_3B + TEAM_BATTING_HR)

df_new$TEAM_BATTING_TOT_BASES <- with(df_new, TEAM_BATTING_1B + (2*TEAM_BATTING_2B) + (3*TEAM_BATTING_3B) + (4*TEAM_BATTING_HR))
```  

For first section - Box plots to visualize each predictor. Note how right-skewed the variables for `TEAM_PITCHING_H`, `TEAM_PITCHING_BB`, `TEAM_PITCHING_SO`, and `TEAM_FIELDING_E` are.

```{r}
# Facet Grid of Box Plots

m <- melt(df_new)
p <- ggplot(m, aes(factor(variable), value)) 
p + geom_boxplot() + facet_wrap(~variable, scale="free")
```  

##Models - Logan 

```{r}

# 1st model uses Total Bases and Extra bases rather than base hits (all base #s)
lt_model1 <- lm(TARGET_WINS ~ TEAM_BATTING_TOT_BASES + TEAM_BATTING_XTRA_BASE + TEAM_BATTING_BB + TEAM_BATTING_SO + TEAM_BASERUN_SB + TEAM_BASERUN_CS + TEAM_PITCHING_H + TEAM_PITCHING_HR + TEAM_PITCHING_BB + TEAM_FIELDING_E + TEAM_FIELDING_DP, df_new) 

# 2nd model uses log transformations on skewed pitching data
lt_model2 <- lm(TARGET_WINS ~ TEAM_BATTING_TOT_BASES + TEAM_BATTING_XTRA_BASE + TEAM_BATTING_BB + TEAM_BATTING_SO + TEAM_BASERUN_SB + TEAM_BASERUN_CS + log1p(TEAM_PITCHING_H) + TEAM_PITCHING_HR + log1p(TEAM_PITCHING_BB) + log(TEAM_FIELDING_E) + TEAM_FIELDING_DP, df_new)

# 3rd model similar to second, removed less significant predictors
lt_model3 <- lm(TARGET_WINS ~ TEAM_BATTING_TOT_BASES + TEAM_BATTING_XTRA_BASE + TEAM_BATTING_BB + TEAM_BATTING_SO + TEAM_BASERUN_SB + TEAM_PITCHING_HR + log(TEAM_FIELDING_E) + TEAM_FIELDING_DP, df_new)
```  

Third model description and methodology.

Many of the more advanced baseball statistics are simply combinations of other statistics (i.e. slugging percentage is total bases divided by at-bats). Using the predictors given in the data set, we wanted to see if combining predictors and/or calculating new values would increase any significance in a model that is trying to predict wins. Total bases and extra bases are both stats that can easily be calculated with the given data. Assuming that the number of doubles, triples and home runs are included in the `TEAM_BATTING_H` variable, we can subtract these out to obtain the number of singles. Adding this to the doubles, triples, and homeruns, each multiplied by the number of bases each is worth (2,3, and 4 respectively) would give total bases. Doing the same thing, but excluding single base hits would give the number of extra bases as well.  These two statistics were used in the model, along with many of the other predictors.
Using a step-wise methodology, less significant predictors such as caught stealing, pitching walks & hits were left out.  To avoid including predictors that were related, the base hits/singles and other multiple base hit predictors were left out. Since the fielding errors predictor was drastically right-skewed, a log transformation was done on this predictor, resulting in a better fit of the model.
Most of the predictors have coefficients that behave the way we would expect, given the predictors effect on the game. The extra bases predictor ended up having a negative coefficient, when we would expect there to be a positive one. This may be due to the fact that it is similar stats that make up these predictors, and there is collinearity between the total bases and extra bases predictors.  Removing extra bases from the model results in a lower coefficient for total bases, and an overall lower adjusted $R^2$ value for the model. 

```{r}
# example of boxcox transformation (not appropriate for predictors with lots of outliers)

boxcox((TARGET_WINS + 1) ~ TEAM_FIELDING_E, data=df_new, lambda = seq(1.2, 1.5, length=10)) #MASS package

summary(lm(TARGET_WINS ~ TEAM_FIELDING_E, df_new))
summary(lm(TARGET_WINS ~ I(TEAM_FIELDING_E^1.35), df_new)) #results in higher adj. R-squared
```