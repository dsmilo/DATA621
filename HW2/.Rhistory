df <- read.csv(url("https://raw.githubusercontent.com/dsmilo/DATA621/master/HW2/Data/classification-output-data.csv"))
head(df)
(confusion <- table(select(df, class, scored.class)))
library(dplyr)
df <- read.csv(url("https://raw.githubusercontent.com/dsmilo/DATA621/master/HW2/Data/classification-output-data.csv"))
head(df)
(confusion <- table(select(df, class, scored.class)))
sensitivity_function <- function(df, predicted){
confusion <- table(select(df, class, get(predicted)))
if(dim(confusion)[2] == 1){
if(colnames(confusion) == 0){
sensitivity <- 0
} else{
sensitivity <- 1
}
} else{
sensitivity <- confusion[2,2]/(confusion[2,1] + confusion[2,2])
}
return(sensitivity)
}
sensitivity_function (df, predicted)
library(dplyr)
df <- read.csv(url("https://raw.githubusercontent.com/dsmilo/DATA621/master/HW2/Data/classification-output-data.csv"))
head(df)
(confusion <- table(select(df, class, scored.class)))
predicted="scored.class"
accuracy_function <- function(df, scored.class){
confusion <- table(select(df, class, scored.class))
accuracy <- (confusion[1,1] + confusion[2,2])/sum(confusion)
return(accuracy)
}
accuracy_function(df,scored.class)
CER_function<- function(df, predicted){
confusion <- table(select(df, class, get(predicted)))
classification_error_rate <- (confusion[1,2] + confusion[2,1])/sum(confusion)
return(classification_error_rate)
}
CER_function(df,predicted)
accuracy_function(df,predicted) +  CER_function(df,predicted)
precision_function <- function(df, predicted){
confusion <- table(select(df, class, get(predicted)))
precision <- confusion[2,2]/(confusion[1,2] + confusion[2,2])
return(precision)
}
precision_function(df,predicted)
sensitivity_function <- function(df, predicted){
confusion <- table(select(df, class, get(predicted)))
if(dim(confusion)[2] == 1){
if(colnames(confusion) == 0){
sensitivity <- 0
} else{
sensitivity <- 1
}
} else{
sensitivity <- confusion[2,2]/(confusion[2,1] + confusion[2,2])
}
return(sensitivity)
}
sensitivity_function (df, predicted)
dim(confusion)
colnames(confusion)
confusion
confusion[2,1]
table(select(df, class, get(predicted)))
dim(confusion)[2]
dim(confusion)
confusion
dim(confusion)
sensitivity_function (df, predicted)
specificity_function(df, predicted)
specificity_function <- function(df,predicted){
confusion <- table(select(df, class, get(predicted)))
if(dim(confusion)[2] == 1){
if(colnames(confusion) == 0){
specificity <- 1
} else{
specificity <- 0
}
} else{
specificity <- confusion[1,1]/(confusion[1,1] + confusion[1,2])
}
return(specificity)
}
specificity_function(df, predicted)
f1score_function <- function(df, predicted){
precision <- precision_function(df, predicted)
sensitivity <- sensitivity_function(df, predicted)
f1score <- (2 * precision * sensitivity)/(precision + sensitivity)
return(f1score)
}
f1score_function(df, predicted)
confusion
precision <- seq(0,1,0.01)
sensitivity <- seq(0,1,0.01)
precision
p <- seq(0,1,0.01)
s <- seq(0,1,0.01)
2*p*s/(p+s)
library(pROC)
roc_curve <- roc(factor(class) ~ scored.probability, data=df)
plot(roc_curve, col="red", main="ROC curve")
```{r}
roc_function <- function(df){
sensitivity_list <- c()
specificity_list <- c()
for (i in seq(0,1, 0.01)) {
df$new.scored <- ifelse(df$scored.probability < i, 0, 1)
sensitivity_list[i*100+1] <- sensitivity_function(df, "new.scored")
specificity_list[i*100+1] <- specificity_function(df, "new.scored")
}
roc_df = data.frame(threshold=seq(0,1, 0.01), sensitivity= sensitivity_list, specificity=specificity_list)
return(roc_df)
}
roc.results <- roc_function(df)
roc.results
dim(df)
seq(0,1, 0.01)
df$scored.probability
head(roc.results)
tail(roc.results)
roc.results <- roc_function(df)
roc.results
library(pROC)
roc_curve <- roc(factor(class) ~ scored.probability, data=df)
plot(roc_curve, col="red", main="ROC curve")
library(pROC)
roc_curve <- roc(factor(class) ~ scored.probability, data=df)
plot(roc_curve, col="red", main="ROC curve")
